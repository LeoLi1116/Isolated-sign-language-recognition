{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"This workbook is divided into several parts, KerasTuner API is not running now to save time, working example is here https://www.kaggle.com/code/aikhmelnytskyy/gislr-tf-on-the-shoulders-ensamble?scriptVersionId=121543912","metadata":{}},{"cell_type":"markdown","source":"# Hi to all! In this notebook, I have prepared an example of how to use the KerasTuner API(https://keras.io/api/keras_tuner/)\n# to find model hyperparameters.\n# I took this wonderful notebook as a basis: https://www.kaggle.com/code/roberthatch/gislr-lb-0-63-on-the-shoulders\n# I added the code from this notebook: https://www.kaggle.com/code/dschettler8845/gislr-how-to-ensemble\n# In version 8, I added the code from this notebook: https://www.kaggle.com/code/aleksandrkruchinin/tflite-ensemble\n# In version 9, I added the code from this notebook: https://www.kaggle.com/code/clemchris/asl-sign-detection-pytorch-lightning\n# Please don't forget to upvote for all notebooks\n# So let's start","metadata":{}},{"cell_type":"markdown","source":"# GISLR [LB 0.63]: On the Shoulders of Giants\nThis notebook is built on top of the great work from the community! In particular, this notebook is copied directly from Darien's excellent work [here][1], and the features are loaded from my notebook [here][2], and that notebook is directly based off of the format [here][3].\n\nThis notebook is going public to celebrate my first ever public LB first place! Fun!\n\nIn this notebook, the main updates are:\n1. Adding the time dimension! Finally, right? There's a 0.58 notebook [here][4] that STILL is just flat mean + std, so I hardly think 0.63 is nearing the limit of what we can achieve. Check out the [features notebook][2], I do two different techniques for the time dimension, resize is one, and padding into a multiple of 3, and then taking mean and std of each third of the data is the other. It took some doing to implement them within the TFLite format/restrictions, so I hope it serves as a useful resource for others!\n2. Just small hyperparameter adjustments.\n3. Oh, and drop face, drop pose, add lips. But refer to the features notebook for the precise details.\n\n# More Credits\nThanks to many Kagglers who have shared ideas. Lonnie's notebook [here][5] helped me and pretty much everyone as we were struggling to figure out this TFLite Model competition format. Andrew helped give suggestions on some of my questions on the discussion page.\n\n[1]: https://www.kaggle.com/code/dschettler8845/gislr-learn-eda-baseline\n[2]: https://www.kaggle.com/code/roberthatch/gislr-feature-data-on-the-shoulders\n[3]: https://www.kaggle.com/code/mayukh18/gislr-feature-data\n[4]: https://www.kaggle.com/code/medali1992/gislr-nn-arcface-baseline\n[5]: https://www.kaggle.com/code/lonnieqin/isolated-sign-language-recognition-with-dnn","metadata":{}},{"cell_type":"markdown","source":"<br><br>\n# MOTIVATION\n**\\#OpenToWork**\n\nAfter 9 years as a Silicon Firmware Engineer at Intel, I am now looking for my next career opportunity. Naturally I am especially interested in Machine Learning opportunities.\n\nIn this notebook, my goals are to:\n* Demonstrate my core strengths. Soft skills like problem solving, communication, and passion.\n* Build up my knowledge skills. I love machine learning and, in particular, love to optimize anything and everything. In this notebook, I explore TensorFlow and Neural Networks.\n* Continue to build on previous Kaggle successes.\n\n**LinkedIn:** https://www.linkedin.com/in/robhatch/\n\n**Kaggle:** https://www.kaggle.com/roberthatch","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nos.environ['TF_CUDNN_DETERMINISTIC'] = '1'","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:29:29.689594Z","iopub.execute_input":"2023-03-23T17:29:29.689993Z","iopub.status.idle":"2023-03-23T17:29:29.717662Z","shell.execute_reply.started":"2023-03-23T17:29:29.689958Z","shell.execute_reply":"2023-03-23T17:29:29.716690Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"print(\"\\n... PIP INSTALLS STARTING ...\\n\")\n!pip install -q --upgrade tensorflow-io\ntry:\n    import mediapipe as mp\nexcept:\n    !pip install -q mediapipe\n    import mediapipe as mp\nprint(\"\\n... PIP INSTALLS COMPLETE ...\\n\")\n\n!pip install lion-tf\nprint(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n# Competition Specific Imports (You'll see why we need these later)\n# mediapipe above\n\n# Machine Learning and Data Science Imports (basics)\nimport tensorflow as tf; print(f\"\\t\\t– TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_io as tfio; print(f\"\\t\\t– TENSORFLOW-IO VERSION: {tfio.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None; pd.set_option('display.max_columns', None);\nimport numpy as np; print(f\"\\t\\t– NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t– SKLEARN VERSION: {sklearn.__version__}\");\n\n# Built-In Imports (mostly don't worry about these)\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom zipfile import ZipFile\nfrom glob import glob\nimport Levenshtein\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_it_all()\n\nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-23T17:29:29.721265Z","iopub.execute_input":"2023-03-23T17:29:29.721714Z","iopub.status.idle":"2023-03-23T17:30:22.250112Z","shell.execute_reply.started":"2023-03-23T17:29:29.721681Z","shell.execute_reply":"2023-03-23T17:30:22.248855Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\n... PIP INSTALLS STARTING ...\n\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\ntensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\ntensorflow-serving-api 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n... PIP INSTALLS COMPLETE ...\n\nCollecting lion-tf\n  Downloading lion-tf-0.0.1.tar.gz (3.2 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tensorflow>=2.11 in /opt/conda/lib/python3.7/site-packages (from lion-tf) (2.11.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (4.4.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (2.2.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (0.31.0)\nRequirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (2.11.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (1.21.6)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (23.1.21)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (3.3.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (0.4.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (1.14.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (3.8.0)\nCollecting protobuf<3.20,>=3.9.2\n  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (23.0)\nRequirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (2.11.2)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (15.0.6.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (1.51.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (1.4.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (59.8.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (0.2.0)\nRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.11->lion-tf) (2.11.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow>=2.11->lion-tf) (0.38.4)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (1.8.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (1.35.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (2.2.3)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (3.4.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (2.28.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (4.11.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (3.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (3.11.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.11->lion-tf) (3.2.2)\nBuilding wheels for collected packages: lion-tf\n  Building wheel for lion-tf (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lion-tf: filename=lion_tf-0.0.1-py3-none-any.whl size=3128 sha256=589e40c8bc644946befbe723e3277bfb84a000d64479f5d77c43e1bed174b611\n  Stored in directory: /root/.cache/pip/wheels/b2/8a/8d/9a0be2470440010954105496dd04d09502b09b51191fd4b9a2\nSuccessfully built lion-tf\nInstalling collected packages: protobuf, lion-tf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 21.12.2 requires cupy-cuda115, which is not installed.\ntfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.79.0 which is incompatible.\ntfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\ntensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\nonnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\napache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed lion-tf-0.0.1 protobuf-3.19.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n... IMPORTS STARTING ...\n\n\n\tVERSION INFORMATION\n\t\t– TENSORFLOW VERSION: 2.11.0\n\t\t– TENSORFLOW-IO VERSION: 0.31.0\n\t\t– NUMPY VERSION: 1.21.6\n\t\t– SKLEARN VERSION: 1.0.2\n\n\n... IMPORTS COMPLETE ...\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SETUP","metadata":{}},{"cell_type":"markdown","source":"### HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def read_json_file(file_path):\n    try:\n        # Open the file and load the JSON data into a Python object\n        with open(file_path, 'r') as file:\n            json_data = json.load(file)\n        return json_data\n    except FileNotFoundError:\n        # Raise an error if the file path does not exist\n        raise FileNotFoundError(f\"File not found: {file_path}\")\n    except ValueError:\n        # Raise an error if the file does not contain valid JSON data\n        raise ValueError(f\"Invalid JSON data in file: {file_path}\")\n\n\nROWS_PER_FRAME = 543  # number of landmarks per frame\ndef load_relevant_data_subset(pq_path):\n    data_columns = ['x', 'y', 'z']\n    data = pd.read_parquet(pq_path, columns=data_columns)\n    n_frames = int(len(data) / ROWS_PER_FRAME)\n    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n    return data.astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:30:22.252129Z","iopub.execute_input":"2023-03-23T17:30:22.252920Z","iopub.status.idle":"2023-03-23T17:30:22.261130Z","shell.execute_reply.started":"2023-03-23T17:30:22.252875Z","shell.execute_reply":"2023-03-23T17:30:22.260051Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### LOAD DATA","metadata":{}},{"cell_type":"code","source":"# Define the path to the root data directory\nDATA_DIR         = \"/kaggle/input/asl-signs\"\n\nprint(\"\\n... BASIC DATA SETUP STARTING ...\\n\")\nprint(\"\\n\\n... LOAD TRAIN DATAFRAME FROM CSV FILE ...\\n\")\n\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\ntrain_df[\"path\"] = DATA_DIR+\"/\"+train_df[\"path\"]\ndisplay(train_df)\n\nprint(\"\\n\\n... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\\n\")\ns2p_map = {k.lower():v for k,v in read_json_file(os.path.join(DATA_DIR, \"sign_to_prediction_index_map.json\")).items()}\np2s_map = {v:k for k,v in read_json_file(os.path.join(DATA_DIR, \"sign_to_prediction_index_map.json\")).items()}\nencoder = lambda x: s2p_map.get(x.lower())\ndecoder = lambda x: p2s_map.get(x)\nprint(s2p_map)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:30:22.264621Z","iopub.execute_input":"2023-03-23T17:30:22.264939Z","iopub.status.idle":"2023-03-23T17:30:22.500473Z","shell.execute_reply.started":"2023-03-23T17:30:22.264906Z","shell.execute_reply":"2023-03-23T17:30:22.499479Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n... BASIC DATA SETUP STARTING ...\n\n\n\n... LOAD TRAIN DATAFRAME FROM CSV FILE ...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                    path  participant_id  \\\n0      /kaggle/input/asl-signs/train_landmark_files/2...           26734   \n1      /kaggle/input/asl-signs/train_landmark_files/2...           28656   \n2      /kaggle/input/asl-signs/train_landmark_files/1...           16069   \n3      /kaggle/input/asl-signs/train_landmark_files/2...           25571   \n4      /kaggle/input/asl-signs/train_landmark_files/6...           62590   \n...                                                  ...             ...   \n94472  /kaggle/input/asl-signs/train_landmark_files/5...           53618   \n94473  /kaggle/input/asl-signs/train_landmark_files/2...           26734   \n94474  /kaggle/input/asl-signs/train_landmark_files/2...           25571   \n94475  /kaggle/input/asl-signs/train_landmark_files/2...           29302   \n94476  /kaggle/input/asl-signs/train_landmark_files/3...           36257   \n\n       sequence_id    sign  \n0       1000035562    blow  \n1       1000106739    wait  \n2        100015657   cloud  \n3       1000210073    bird  \n4       1000240708    owie  \n...            ...     ...  \n94472    999786174   white  \n94473    999799849    have  \n94474    999833418  flower  \n94475    999895257    room  \n94476    999962374   happy  \n\n[94477 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/1...</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/6...</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>94472</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/5...</td>\n      <td>53618</td>\n      <td>999786174</td>\n      <td>white</td>\n    </tr>\n    <tr>\n      <th>94473</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n      <td>26734</td>\n      <td>999799849</td>\n      <td>have</td>\n    </tr>\n    <tr>\n      <th>94474</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n      <td>25571</td>\n      <td>999833418</td>\n      <td>flower</td>\n    </tr>\n    <tr>\n      <th>94475</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n      <td>29302</td>\n      <td>999895257</td>\n      <td>room</td>\n    </tr>\n    <tr>\n      <th>94476</th>\n      <td>/kaggle/input/asl-signs/train_landmark_files/3...</td>\n      <td>36257</td>\n      <td>999962374</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n<p>94477 rows × 4 columns</p>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n\n... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\n\n{'tv': 0, 'after': 1, 'airplane': 2, 'all': 3, 'alligator': 4, 'animal': 5, 'another': 6, 'any': 7, 'apple': 8, 'arm': 9, 'aunt': 10, 'awake': 11, 'backyard': 12, 'bad': 13, 'balloon': 14, 'bath': 15, 'because': 16, 'bed': 17, 'bedroom': 18, 'bee': 19, 'before': 20, 'beside': 21, 'better': 22, 'bird': 23, 'black': 24, 'blow': 25, 'blue': 26, 'boat': 27, 'book': 28, 'boy': 29, 'brother': 30, 'brown': 31, 'bug': 32, 'bye': 33, 'callonphone': 34, 'can': 35, 'car': 36, 'carrot': 37, 'cat': 38, 'cereal': 39, 'chair': 40, 'cheek': 41, 'child': 42, 'chin': 43, 'chocolate': 44, 'clean': 45, 'close': 46, 'closet': 47, 'cloud': 48, 'clown': 49, 'cow': 50, 'cowboy': 51, 'cry': 52, 'cut': 53, 'cute': 54, 'dad': 55, 'dance': 56, 'dirty': 57, 'dog': 58, 'doll': 59, 'donkey': 60, 'down': 61, 'drawer': 62, 'drink': 63, 'drop': 64, 'dry': 65, 'dryer': 66, 'duck': 67, 'ear': 68, 'elephant': 69, 'empty': 70, 'every': 71, 'eye': 72, 'face': 73, 'fall': 74, 'farm': 75, 'fast': 76, 'feet': 77, 'find': 78, 'fine': 79, 'finger': 80, 'finish': 81, 'fireman': 82, 'first': 83, 'fish': 84, 'flag': 85, 'flower': 86, 'food': 87, 'for': 88, 'frenchfries': 89, 'frog': 90, 'garbage': 91, 'gift': 92, 'giraffe': 93, 'girl': 94, 'give': 95, 'glasswindow': 96, 'go': 97, 'goose': 98, 'grandma': 99, 'grandpa': 100, 'grass': 101, 'green': 102, 'gum': 103, 'hair': 104, 'happy': 105, 'hat': 106, 'hate': 107, 'have': 108, 'haveto': 109, 'head': 110, 'hear': 111, 'helicopter': 112, 'hello': 113, 'hen': 114, 'hesheit': 115, 'hide': 116, 'high': 117, 'home': 118, 'horse': 119, 'hot': 120, 'hungry': 121, 'icecream': 122, 'if': 123, 'into': 124, 'jacket': 125, 'jeans': 126, 'jump': 127, 'kiss': 128, 'kitty': 129, 'lamp': 130, 'later': 131, 'like': 132, 'lion': 133, 'lips': 134, 'listen': 135, 'look': 136, 'loud': 137, 'mad': 138, 'make': 139, 'man': 140, 'many': 141, 'milk': 142, 'minemy': 143, 'mitten': 144, 'mom': 145, 'moon': 146, 'morning': 147, 'mouse': 148, 'mouth': 149, 'nap': 150, 'napkin': 151, 'night': 152, 'no': 153, 'noisy': 154, 'nose': 155, 'not': 156, 'now': 157, 'nuts': 158, 'old': 159, 'on': 160, 'open': 161, 'orange': 162, 'outside': 163, 'owie': 164, 'owl': 165, 'pajamas': 166, 'pen': 167, 'pencil': 168, 'penny': 169, 'person': 170, 'pig': 171, 'pizza': 172, 'please': 173, 'police': 174, 'pool': 175, 'potty': 176, 'pretend': 177, 'pretty': 178, 'puppy': 179, 'puzzle': 180, 'quiet': 181, 'radio': 182, 'rain': 183, 'read': 184, 'red': 185, 'refrigerator': 186, 'ride': 187, 'room': 188, 'sad': 189, 'same': 190, 'say': 191, 'scissors': 192, 'see': 193, 'shhh': 194, 'shirt': 195, 'shoe': 196, 'shower': 197, 'sick': 198, 'sleep': 199, 'sleepy': 200, 'smile': 201, 'snack': 202, 'snow': 203, 'stairs': 204, 'stay': 205, 'sticky': 206, 'store': 207, 'story': 208, 'stuck': 209, 'sun': 210, 'table': 211, 'talk': 212, 'taste': 213, 'thankyou': 214, 'that': 215, 'there': 216, 'think': 217, 'thirsty': 218, 'tiger': 219, 'time': 220, 'tomorrow': 221, 'tongue': 222, 'tooth': 223, 'toothbrush': 224, 'touch': 225, 'toy': 226, 'tree': 227, 'uncle': 228, 'underwear': 229, 'up': 230, 'vacuum': 231, 'wait': 232, 'wake': 233, 'water': 234, 'wet': 235, 'weus': 236, 'where': 237, 'white': 238, 'who': 239, 'why': 240, 'will': 241, 'wolf': 242, 'yellow': 243, 'yes': 244, 'yesterday': 245, 'yourself': 246, 'yucky': 247, 'zebra': 248, 'zipper': 249}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PREPROCESSING\n\nThe preprocessing cells need to match the same cells from the feature generation notebook","metadata":{}},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"DROP_Z = False\n\nNUM_FRAMES1 = 15\nSEGMENTS = 3\n\nLEFT_HAND_OFFSET = 468\nPOSE_OFFSET = LEFT_HAND_OFFSET+21\nRIGHT_HAND_OFFSET = POSE_OFFSET+33\n\n## average over the entire face, and the entire 'pose'\naveraging_sets = [[0, 468], [POSE_OFFSET, 33]]\n\nlip_landmarks = [61, 185, 40, 39, 37,  0, 267, 269, 270, 409,\n                 291,146, 91,181, 84, 17, 314, 405, 321, 375, \n                 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, \n                 95, 88, 178, 87, 14,317, 402, 318, 324, 308]\nleft_hand_landmarks = list(range(LEFT_HAND_OFFSET, LEFT_HAND_OFFSET+21))\nright_hand_landmarks = list(range(RIGHT_HAND_OFFSET, RIGHT_HAND_OFFSET+21))\n\npoint_landmarks = [item for sublist in [lip_landmarks, left_hand_landmarks, right_hand_landmarks] for item in sublist]\n\nLANDMARKS1 = len(point_landmarks) + len(averaging_sets)\nprint(LANDMARKS1)\nif DROP_Z:\n    INPUT_SHAPE1 = (NUM_FRAMES1,LANDMARKS1*2)\nelse:\n    INPUT_SHAPE1 = (NUM_FRAMES1,LANDMARKS1*3)\n\nFLAT_INPUT_SHAPE1 = (INPUT_SHAPE1[0] + 2 * (SEGMENTS + 1)) * INPUT_SHAPE1[1]\n    ","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:30:22.503610Z","iopub.execute_input":"2023-03-23T17:30:22.503925Z","iopub.status.idle":"2023-03-23T17:30:22.512994Z","shell.execute_reply.started":"2023-03-23T17:30:22.503897Z","shell.execute_reply":"2023-03-23T17:30:22.511955Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"84\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"def tf_nan_mean(x, axis=0):\n    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)\n\ndef tf_nan_std(x, axis=0):\n    d = x - tf_nan_mean(x, axis=axis)\n    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis))\n\ndef flatten_means_and_stds1(x, axis=0):\n    # Get means and stds\n    x_mean = tf_nan_mean(x, axis=0)\n    x_std  = tf_nan_std(x,  axis=0)\n\n    x_out = tf.concat([x_mean, x_std], axis=0)\n    x_out = tf.reshape(x_out, (1, INPUT_SHAPE1[1]*2))\n    x_out = tf.where(tf.math.is_finite(x_out), x_out, tf.zeros_like(x_out))\n    return x_out\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:30:22.514599Z","iopub.execute_input":"2023-03-23T17:30:22.515405Z","iopub.status.idle":"2023-03-23T17:30:22.527498Z","shell.execute_reply.started":"2023-03-23T17:30:22.515369Z","shell.execute_reply":"2023-03-23T17:30:22.526503Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## TensorFlow Feature Preprocessing Layer","metadata":{}},{"cell_type":"code","source":"class FeatureGen(tf.keras.layers.Layer):\n    def __init__(self):\n        super(FeatureGen, self).__init__()\n    \n    def call(self, x_in):\n        if DROP_Z:\n            x_in = x_in[:, :, 0:2]\n        x_list = [tf.expand_dims(tf_nan_mean(x_in[:, av_set[0]:av_set[0]+av_set[1], :], axis=1), axis=1) for av_set in averaging_sets]\n        x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n        x = tf.concat(x_list, 1)\n\n        x_padded = x\n        for i in range(SEGMENTS):\n            p0 = tf.where( ((tf.shape(x_padded)[0] % SEGMENTS) > 0) & ((i % 2) != 0) , 1, 0)\n            p1 = tf.where( ((tf.shape(x_padded)[0] % SEGMENTS) > 0) & ((i % 2) == 0) , 1, 0)\n            paddings = [[p0, p1], [0, 0], [0, 0]]\n            x_padded = tf.pad(x_padded, paddings, mode=\"SYMMETRIC\")\n        x_list = tf.split(x_padded, SEGMENTS)\n        x_list = [flatten_means_and_stds1(_x, axis=0) for _x in x_list]\n\n        x_list.append(flatten_means_and_stds1(x, axis=0))\n        \n        ## Resize only dimension 0. Resize can't handle nan, so replace nan with that dimension's avg value to reduce impact.\n        x = tf.image.resize(tf.where(tf.math.is_finite(x), x, tf_nan_mean(x, axis=0)), [NUM_FRAMES1, LANDMARKS1])\n        x = tf.reshape(x, (1, INPUT_SHAPE1[0]*INPUT_SHAPE1[1]))\n        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n        x_list.append(x)\n        x = tf.concat(x_list, axis=1)\n        return x\n\nprint(FeatureGen()(tf.keras.Input((543, 3), dtype=tf.float32, name=\"inputs\")))\nFeatureGen()(load_relevant_data_subset(train_df.path[0]))","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:30:22.529085Z","iopub.execute_input":"2023-03-23T17:30:22.529513Z","iopub.status.idle":"2023-03-23T17:30:25.842508Z","shell.execute_reply.started":"2023-03-23T17:30:22.529477Z","shell.execute_reply":"2023-03-23T17:30:25.841312Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"KerasTensor(type_spec=TensorSpec(shape=(1, 5796), dtype=tf.float32, name=None), name='feature_gen/concat_5:0', description=\"created by layer 'feature_gen'\")\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 5796), dtype=float32, numpy=\narray([[ 5.18924236e-01,  3.42620254e-01,  1.48732506e-05, ...,\n         6.35599867e-02,  5.70323110e-01, -1.20788895e-01]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"markdown","source":"# TRAIN","metadata":{}},{"cell_type":"code","source":"## Hyperparameters\nBATCH_SIZE = 128\nVAL_PCT = 0.1\nLEARNING_RATE = 0.000333\nLR_PATIENCE = 2\nLR_REDUCTION_FACTOR = 0.8\nEPOCHS = 100\n\nSTARTING_LAYER_SIZE = 1024\nDROPOUTS = [0.4, 0.4]\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:30:25.845712Z","iopub.execute_input":"2023-03-23T17:30:25.846029Z","iopub.status.idle":"2023-03-23T17:30:25.854798Z","shell.execute_reply.started":"2023-03-23T17:30:25.845998Z","shell.execute_reply":"2023-03-23T17:30:25.853758Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_x    = np.load(\"/kaggle/input/gislr-feature-data-on-the-shoulders/feature_data.npy\").astype(np.float32)\ntrain_y    = np.load(\"/kaggle/input/gislr-feature-data-on-the-shoulders/feature_labels.npy\").astype(np.uint8)\nprint(train_x.shape, train_y.shape)\nif DROP_Z:\n    train_x = np.reshape(train_x, [train_x.shape[0], -1, 3])\n    train_x = train_x[:, :, 0:2]\n    train_x = np.reshape(train_x, [train_x.shape[0], -1])\n    print(train_x.shape, train_y.shape)\n\nN_TOTAL = train_x.shape[0]\nFLAT_FRAME_SHAPE = train_x.shape[1]\nassert(FLAT_FRAME_SHAPE == FLAT_INPUT_SHAPE1)\nN_VAL   = int(N_TOTAL*VAL_PCT)\nN_TRAIN = N_TOTAL-N_VAL\n\n#random_idxs = random.sample(range(N_TOTAL), N_TOTAL)\n#train_idxs, val_idxs = np.array(random_idxs[:N_TRAIN]), np.array(random_idxs[N_TRAIN:])\n\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5)\nfold=0\nfor i, (train_index, test_index) in enumerate(skf.split(train_x,train_y)):\n    print(f\"Fold {i}:\")\n    if i==fold:\n        #print(train_index)\n        val_x_fold, val_y_fold = train_x[test_index], train_y[test_index]\n        train_x_fold, train_y_fold = train_x[train_index], train_y[train_index]\n\ntrain_x_fold.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:30:25.857024Z","iopub.execute_input":"2023-03-23T17:30:25.857694Z","iopub.status.idle":"2023-03-23T17:31:01.678237Z","shell.execute_reply.started":"2023-03-23T17:30:25.857624Z","shell.execute_reply":"2023-03-23T17:31:01.677030Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(94477, 5796) (94477,)\nFold 0:\nFold 1:\nFold 2:\nFold 3:\nFold 4:\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(75581, 5796)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing kerastuner.tuners","metadata":{}},{"cell_type":"code","source":"\nfrom kerastuner.tuners import RandomSearch, BayesianOptimization\nfrom lion_tf import Lion","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:01.683108Z","iopub.execute_input":"2023-03-23T17:31:01.683574Z","iopub.status.idle":"2023-03-23T17:31:01.792842Z","shell.execute_reply.started":"2023-03-23T17:31:01.683537Z","shell.execute_reply":"2023-03-23T17:31:01.790746Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n  \"\"\"Entry point for launching an IPython kernel.\n","output_type":"stream"}]},{"cell_type":"code","source":"def fc_block(inputs, output_channels, dropout=0.2, activation=\"gelu\"):\n    x = tf.keras.layers.Dense(output_channels)(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(activation)(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n    return x\ndef build_model(hp):\n    \n    \n    #Here we set the hyperparameters for the search, this is just a basic example:    \n    STARTING_LAYER_SIZE = hp.Int(name=\"STARTING_LAYER_SIZE\", min_value=1426, max_value=1554, step=32)\n    dropout = [hp.Float(name=\"dropout1\", min_value=0.24, max_value=0.40, step=0.02),   \n               hp.Float(name=\"dropout2\", min_value=0.24, max_value=0.40, step=0.02)]\n    DROPOUTS_len= 2#hp.Int(name=\"dropouts_len\", min_value=2, max_value=5, step=1)\n    hp_activation = hp.Choice('hp_activation', values=[\"gelu\", \"relu\"])\n    hp_learning_rate = hp.Choice('hp_learning_rate', values=[LEARNING_RATE,LEARNING_RATE*0.7, LEARNING_RATE*0.9])\n    \n    hp_optimizer = hp.Choice('optimizer', values=['adam'])\n\n    if hp_optimizer == 'lion':\n        optimizer = Lion(hp_learning_rate)\n    elif hp_optimizer == 'adam':\n        optimizer = tf.keras.optimizers.Adam(hp_learning_rate)\n    def get_model(n_labels=250, init_fc=STARTING_LAYER_SIZE, flat_frame_len=FLAT_FRAME_SHAPE):\n        _inputs = tf.keras.layers.Input(shape=(flat_frame_len,))\n        x = _inputs\n\n        # Define layers\n        \n        \n        for i in range(DROPOUTS_len):\n            x = fc_block(\n                x, output_channels=init_fc//(2**i), \n                dropout=dropout[i]\n            )\n\n        # Define output layer\n        _outputs = tf.keras.layers.Dense(n_labels, activation=\"softmax\")(x)\n\n        # Build the model\n        model = tf.keras.models.Model(inputs=_inputs, outputs=_outputs)\n        return model\n\n    model = get_model()\n    model.compile(optimizer, \"sparse_categorical_crossentropy\", metrics=\"acc\")\n    model.summary()\n    return model\n    #tf.keras.utils.plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:01.794311Z","iopub.execute_input":"2023-03-23T17:31:01.794909Z","iopub.status.idle":"2023-03-23T17:31:01.807632Z","shell.execute_reply.started":"2023-03-23T17:31:01.794868Z","shell.execute_reply":"2023-03-23T17:31:01.806611Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#strategy = tf.distribute.MirroredStrategy()\ntuner = BayesianOptimization(\n    build_model,\n    objective='val_acc',\n    max_trials=20,\n    executions_per_trial=1,\n    directory='/',\n    project_name='asl',\n    #distribution_strategy=strategy\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:01.809530Z","iopub.execute_input":"2023-03-23T17:31:01.811078Z","iopub.status.idle":"2023-03-23T17:31:01.967813Z","shell.execute_reply.started":"2023-03-23T17:31:01.811036Z","shell.execute_reply":"2023-03-23T17:31:01.966999Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 5796)]            0         \n                                                                 \n dense (Dense)               (None, 1426)              8266522   \n                                                                 \n batch_normalization (BatchN  (None, 1426)             5704      \n ormalization)                                                   \n                                                                 \n activation (Activation)     (None, 1426)              0         \n                                                                 \n dropout (Dropout)           (None, 1426)              0         \n                                                                 \n dense_1 (Dense)             (None, 713)               1017451   \n                                                                 \n batch_normalization_1 (Batc  (None, 713)              2852      \n hNormalization)                                                 \n                                                                 \n activation_1 (Activation)   (None, 713)               0         \n                                                                 \n dropout_1 (Dropout)         (None, 713)               0         \n                                                                 \n dense_2 (Dense)             (None, 250)               178500    \n                                                                 \n=================================================================\nTotal params: 9,471,029\nTrainable params: 9,466,751\nNon-trainable params: 4,278\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir models\ncb_list = [\n    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(patience=LR_PATIENCE, factor=LR_REDUCTION_FACTOR, verbose=1)\n]\n#history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=EPOCHS, callbacks=cb_list, batch_size=BATCH_SIZE)\n#model.save(\"./models/asl_model\")","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:01.968889Z","iopub.execute_input":"2023-03-23T17:31:01.969252Z","iopub.status.idle":"2023-03-23T17:31:03.001981Z","shell.execute_reply.started":"2023-03-23T17:31:01.969213Z","shell.execute_reply":"2023-03-23T17:31:03.000633Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#We are looking for hyperparameters. Note that I only use 10 epochs, the number of epochs can be increased\nsearch=False\nif search:\n    tuner.search(train_x_fold, train_y_fold, epochs=100, validation_data=(val_x_fold, val_y_fold), callbacks=cb_list, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:03.004191Z","iopub.execute_input":"2023-03-23T17:31:03.004586Z","iopub.status.idle":"2023-03-23T17:31:03.011053Z","shell.execute_reply.started":"2023-03-23T17:31:03.004542Z","shell.execute_reply":"2023-03-23T17:31:03.009734Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"if search:\n    best_hps = tuner.get_best_hyperparameters()[0].values\n    print('The best hyperparameters:',best_hps)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:03.012900Z","iopub.execute_input":"2023-03-23T17:31:03.013289Z","iopub.status.idle":"2023-03-23T17:31:03.020752Z","shell.execute_reply.started":"2023-03-23T17:31:03.013253Z","shell.execute_reply":"2023-03-23T17:31:03.019678Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"if search:\n    STARTING_LAYER_SIZE = best_hps['STARTING_LAYER_SIZE']\n    dropout = [best_hps['dropout1'],best_hps['dropout2']]\n    DROPOUTS_len= 2#best_hps['dropouts_len']\n    hp_activation=best_hps['hp_activation']\n    hp_learning_rate=best_hps['hp_learning_rate']\n    hp_optimizer=best_hps['optimizer']\nelse:\n    #0.7616174221038818\n    STARTING_LAYER_SIZE = 1426#1024\n    dropout = [0.3,0.3]\n    DROPOUTS_len= 2#3\n    hp_activation='relu'#'gelu'\n    hp_learning_rate=0.000333#LEARNING_RATE\n    hp_optimizer='adam'#'adam'\n    \n    \nif hp_optimizer == 'lion':\n        optimizer = Lion(hp_learning_rate)\nelif hp_optimizer == 'adam':\n    optimizer = tf.keras.optimizers.Adam(hp_learning_rate)    \n    \ndef get_model(n_labels=250, init_fc=STARTING_LAYER_SIZE, flat_frame_len=FLAT_FRAME_SHAPE):\n    _inputs = tf.keras.layers.Input(shape=(flat_frame_len,))\n    x = _inputs\n\n    # Define layers\n\n    for i in range(DROPOUTS_len):\n        x = fc_block(\n            x, output_channels=init_fc//(2**i), \n            dropout=dropout[i]\n        )\n    # Define output layer\n    _outputs = tf.keras.layers.Dense(n_labels, activation=\"softmax\")(x)\n\n    # Build the model\n    model = tf.keras.models.Model(inputs=_inputs, outputs=_outputs)\n    return model\n\nmodel = get_model()\nmodel.compile(optimizer, \"sparse_categorical_crossentropy\", metrics=\"acc\")\nmodel.summary()\ntf.keras.utils.plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:03.022200Z","iopub.execute_input":"2023-03-23T17:31:03.022720Z","iopub.status.idle":"2023-03-23T17:31:03.503103Z","shell.execute_reply.started":"2023-03-23T17:31:03.022685Z","shell.execute_reply":"2023-03-23T17:31:03.501839Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 5796)]            0         \n                                                                 \n dense_3 (Dense)             (None, 1426)              8266522   \n                                                                 \n batch_normalization_2 (Batc  (None, 1426)             5704      \n hNormalization)                                                 \n                                                                 \n activation_2 (Activation)   (None, 1426)              0         \n                                                                 \n dropout_2 (Dropout)         (None, 1426)              0         \n                                                                 \n dense_4 (Dense)             (None, 713)               1017451   \n                                                                 \n batch_normalization_3 (Batc  (None, 713)              2852      \n hNormalization)                                                 \n                                                                 \n activation_3 (Activation)   (None, 713)               0         \n                                                                 \n dropout_3 (Dropout)         (None, 713)               0         \n                                                                 \n dense_5 (Dense)             (None, 250)               178500    \n                                                                 \n=================================================================\nTotal params: 9,471,029\nTrainable params: 9,466,751\nNon-trainable params: 4,278\n_________________________________________________________________\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"image/png":"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\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}]},{"cell_type":"code","source":"if_train=False\nimport gc\ngc.collect()\nfolds=[0,1,2,3,4]\nmodels=[]\nfor fold in folds:\n    print(f\"Fold {fold}:\")\n    cb_list = [\n    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(patience=LR_PATIENCE, factor=LR_REDUCTION_FACTOR, verbose=1)]\n    for i, (train_index, test_index) in enumerate(skf.split(train_x,train_y)):\n        if i==fold:\n            val_x_fold, val_y_fold = train_x[test_index], train_y[test_index]\n            train_x_fold, train_y_fold = train_x[train_index], train_y[train_index]\n        \n    model = get_model()\n    if hp_optimizer == 'lion':\n        optimizer = Lion(hp_learning_rate)\n    elif hp_optimizer == 'adam':\n        optimizer = tf.keras.optimizers.Adam(hp_learning_rate)    \n    model.compile(optimizer, \"sparse_categorical_crossentropy\", metrics=\"acc\")\n    #model.summary()\n    #tf.keras.utils.plot_model(model)\n    if if_train:\n        history = model.fit(train_x_fold, train_y_fold, validation_data=(val_x_fold, val_y_fold), epochs=EPOCHS, callbacks=cb_list, batch_size=BATCH_SIZE)#\n    else:\n        model = tf.keras.models.load_model(f'/kaggle/input/gislr-tf-on-the-shoulders-kerastunerversion14/models/asl_model_{fold}')#The models are trained in https://www.kaggle.com/code/aikhmelnytskyy/gislr-tf-on-the-shoulders-ensamble?scriptVersionId=121543912\n        model.evaluate(val_x_fold, val_y_fold)\n    model.save(f\"./models/asl_model_{fold}\")\n    \n    #model.evaluate(val_x_fold, val_y_fold)\n    #for x,y in zip(val_x_fold[:10], val_y_fold[:10]):\n    #    print(f\"PRED: {decoder(np.argmax(model.predict(tf.expand_dims(x, axis=0), verbose=0), axis=-1)[0]):<20} – GT: {decoder(y)}\")\n    models.append(model)\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:03.505057Z","iopub.execute_input":"2023-03-23T17:31:03.505379Z","iopub.status.idle":"2023-03-23T17:31:47.724302Z","shell.execute_reply.started":"2023-03-23T17:31:03.505346Z","shell.execute_reply":"2023-03-23T17:31:47.723234Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Fold 0:\n591/591 [==============================] - 4s 3ms/step - loss: 1.0884 - acc: 0.7410\nFold 1:\n591/591 [==============================] - 3s 4ms/step - loss: 1.0752 - acc: 0.7453\nFold 2:\n591/591 [==============================] - 2s 3ms/step - loss: 1.0413 - acc: 0.7458\nFold 3:\n591/591 [==============================] - 2s 3ms/step - loss: 1.0578 - acc: 0.7470\nFold 4:\n591/591 [==============================] - 3s 4ms/step - loss: 1.0835 - acc: 0.7399\n","output_type":"stream"}]},{"cell_type":"code","source":"#model=build_model(best_hps) \n#model=get_model()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:47.728925Z","iopub.execute_input":"2023-03-23T17:31:47.729307Z","iopub.status.idle":"2023-03-23T17:31:47.734295Z","shell.execute_reply.started":"2023-03-23T17:31:47.729269Z","shell.execute_reply":"2023-03-23T17:31:47.733163Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#We train the model from scratch, please note that if you have trained enough epochs of the model during meshing, this step can be replaced by\n#model = tuner.get_best_models(num_models=1)[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:47.736141Z","iopub.execute_input":"2023-03-23T17:31:47.736879Z","iopub.status.idle":"2023-03-23T17:31:47.744060Z","shell.execute_reply.started":"2023-03-23T17:31:47.736840Z","shell.execute_reply":"2023-03-23T17:31:47.742990Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#DEFINE A MODEL FUNCTION AND HYPERPARAMETERS\nclass EpochPrintCB(tf.keras.callbacks.Callback):\n    def __init__(self, n_epochs_btwn_prints=5, extra_metrics_to_incl=None):\n        self.n_epochs_btwn_prints=n_epochs_btwn_prints        \n        self.extra_metrics_to_incl = extra_metrics_to_incl if ((extra_metrics_to_incl is None) or (type(extra_metrics_to_incl)==list)) else list(extra_metrics_to_incl)\n    \n    def on_epoch_end(self, epoch, logs):\n        if epoch % self.n_epochs_btwn_prints == 0:\n            print_str = f\"|| Epoch {epoch:>3} | lr: {self.model.optimizer.lr.numpy():10.7f} || loss:{logs['loss']:8.5f} | acc:{logs['acc']:8.5f} || val_loss:{logs['val_loss']:8.5f} | val_acc:{logs['val_acc']:8.5f} ||\"\n            if self.extra_metrics_to_incl is not None:\n                for extra_metric in self.extra_metrics_to_incl:\n                        print_str = \"||\".join([\n                            group if i in [0, 1, len(print_str.split(\"||\"))-1] else group[:-1]+f\" | {'val_' if group[1]=='v' else ''}{extra_metric}:{logs[('val_' if group[1]=='v' else '')+extra_metric]:8.5f} \"\n                            for i, group in enumerate(print_str.split(\"||\"))\n                        ])\n            print(print_str)\n\ndef fc_block(inputs, output_channels, dropout=0.2, _act=\"relu\"):\n    x = tf.keras.layers.Dense(output_channels)(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation(_act)(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n    return x\n\ndef get_model(n_labels=250, init_fc=512, n_blocks=2, _dropout_1=0.2, _dropout_2=0.6, _fc_step_rate=2, n_ax=2, n_feats=2,\n              types_to_use=(\"lips\", \"left_hand\", \"pose\", \"right_hand\"), do_L1=False, do_L2=False,\n              type_frame_len={\"lips\":43, \"left_hand\":21, \"pose\":33, \"right_hand\":21}):\n    \n    flat_frame_len = sum([type_frame_len[x]*n_ax*n_feats for x in types_to_use])\n    _inputs = tf.keras.layers.Input(shape=(flat_frame_len,))\n    x = _inputs\n    \n    # Define layers\n    for i in range(n_blocks):\n        x = fc_block(\n            x, output_channels=init_fc//(_fc_step_rate**i),\n            dropout=_dropout_1 if i!=(n_blocks-1) else _dropout_2\n        )\n    \n    # Define output layers\n    _outputs = tf.keras.layers.Dense(n_labels, activation=\"softmax\")(x)\n    \n    # Build the model\n    model = tf.keras.models.Model(inputs=_inputs, outputs=_outputs)\n    return model\n\nBATCH_SIZE   = 2048\nLR           = 0.0005\nDROPOUT_1    = 0.3\nDROPOUT_2    = 0.5\nN_EPOCHS     = 400\nINIT_FC      = 512\nN_BLOCKS     = 2\nFC_STEP_RATE = 1.25\nCB_MONITOR   = \"val_acc\"\nLOSS_FN      = \"sparse_categorical_crossentropy\"\nMETRICS      = [\"acc\", tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='t5_acc')]\n\nmodel_kwargs =dict(\n    init_fc=INIT_FC, \n    n_blocks=N_BLOCKS, \n    _dropout_1=DROPOUT_1, \n    _dropout_2=DROPOUT_2, \n    _fc_step_rate=FC_STEP_RATE,\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:47.745846Z","iopub.execute_input":"2023-03-23T17:31:47.746287Z","iopub.status.idle":"2023-03-23T17:31:47.767746Z","shell.execute_reply.started":"2023-03-23T17:31:47.746252Z","shell.execute_reply":"2023-03-23T17:31:47.766620Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"del train_x,train_y\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:47.769858Z","iopub.execute_input":"2023-03-23T17:31:47.770529Z","iopub.status.idle":"2023-03-23T17:31:48.122255Z","shell.execute_reply.started":"2023-03-23T17:31:47.770488Z","shell.execute_reply":"2023-03-23T17:31:48.121171Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"23"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\nNP_FILE_DIR      = \"/kaggle/input/isolated-sign-language-aggregation-preparation\"\nall_x = np.load(os.path.join(NP_FILE_DIR, \"feature_data.npy\")).astype(np.float32)\nall_y = np.load(os.path.join(NP_FILE_DIR, \"feature_labels.npy\")).astype(np.uint8)\n\n# add nan back in not to screw up means/std\nall_x = np.where(all_x==0.0, np.nan, all_x)\n\n# Get mean and std ignoring nans\nall_mean = np.nanmean(all_x, keepdims=True, axis=0)\nall_std = np.nanstd(all_x, keepdims=True, axis=0)\n\n# Standardize around 0\nall_x = (all_x-all_mean)/all_std\n# Technically I don't need to do the were because np.nan \n# subtracting or dividing anything still results in np.nan\n#    - all_x = np.where(np.isnan(all_x), all_x, all_x-all_mean)\n#    - all_x = np.where(np.isnan(all_x), all_x, all_x/all_std)\n\n# Back to 0s\nall_x = np.nan_to_num(all_x)\n\n# There are 21 participants so we use 7 folds\n#    --> i.e. 3 participants in val every time\nN_PARTICIPANTS = train_df.participant_id.nunique()\n\n# Thanks Robert Hatch! \n#     --> https://www.kaggle.com/competitions/asl-signs/discussion/392335\n#\n# We are including 29302 in RH signer\nRH_SIGNERS = [26734, 28656, 25571, 62590, 29302, \n                       49445, 53618, 18796,  4718,  2044, \n                       37779, 30680]\n\n# We are including 37055 in LH Signer\nLH_SIGNERS  = [16069, 32319, 36257, 22343, 27610, 61333, 34503, 55372, 37055]\n\n# Since the sign label itself if relatively \n# There are 21 participants so we use 7 folds\n#    --> i.e. 3 participants in val every time\nK_FOLDS = 7\n\ndef get_folds(df, k_folds, force_one_lh=False, lh_signers=LH_SIGNERS[:-1]):\n    while True:\n        sgkf = StratifiedGroupKFold(n_splits=K_FOLDS, shuffle=True)\n        _fold_ds_idx_map = {\n            i:{\"train\":t_idxs, \"val\":v_idxs} \\\n            for i, (t_idxs, v_idxs) in enumerate(sgkf.split(df.index, df.sign, df.participant_id))\n        }\n        \n        # Ensure only one left hander in every val group\n        if force_one_lh:\n            if all([len(set(df.iloc[_idxs['val']][\"participant_id\"].unique()).intersection(set(lh_signers)))==1 for _idxs in _fold_ds_idx_map.values()]):\n                return _fold_ds_idx_map\n            else:\n                print(\".\", end=\"\")\n        else:\n            return _fold_ds_idx_map\n    \nfold_ds_idx_map = get_folds(train_df, K_FOLDS)\nprint(\" APPROPRIATE KFOLD SPLIT FOUND!\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:48.124426Z","iopub.execute_input":"2023-03-23T17:31:48.124932Z","iopub.status.idle":"2023-03-23T17:31:52.348138Z","shell.execute_reply.started":"2023-03-23T17:31:48.124892Z","shell.execute_reply":"2023-03-23T17:31:52.346207Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":" APPROPRIATE KFOLD SPLIT FOUND!\n\n","output_type":"stream"}]},{"cell_type":"code","source":"histories, MODEL_DIR = [], \"/kaggle/working/models1\"\nif not os.path.isdir(MODEL_DIR): os.makedirs(MODEL_DIR)\n\nfor fold_num, fold_idxs in fold_ds_idx_map.items():    \n    print(f\"\\n\\n... STARTING TRAINING FOR FOLD #{fold_num+1} ...\\n\")\n    \n    # Get the dataset\n    val_x, val_y     = all_x[fold_idxs[\"val\"]],   all_y[fold_idxs[\"val\"]]\n    train_x, train_y = all_x[fold_idxs[\"train\"]], all_y[fold_idxs[\"train\"]]\n\n    # Initialize optimizer\n    optimizer = tf.keras.optimizers.Adam(LR)\n    \n    # Initialize CB list\n    _pct_to_drop = 3\n    cb_list = [\n        tf.keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True, verbose=1, monitor=CB_MONITOR),\n        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=(1-0.01*_pct_to_drop), verbose=0, monitor=CB_MONITOR),\n        EpochPrintCB(extra_metrics_to_incl=[\"t5_acc\",])\n    ]\n            \n    # Initialize model\n    model = get_model(**model_kwargs)\n    model.compile(optimizer, loss=LOSS_FN, metrics=METRICS)\n    \n    # See the structure and number of parameters\n    if fold_num==0: print(f\"\\n\\nFIRST FOLD... PRINTING MODEL SUMMARY:\\n\"); model.summary()\n        \n    # Fit!\n    print(\"\\n\\n... BEGINNING MODEL TRAINING ...\\n\")\n    if if_train:\n        histories.append(model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=N_EPOCHS, callbacks=cb_list, batch_size=BATCH_SIZE, verbose=0))\n    \n    else:\n        model = tf.keras.models.load_model(f'/kaggle/input/gislr-how-to-ensemble-version9/results/models/islr_model__fold_0{fold_num+1}')#The models are trained in version 14\n    \n    # Save\n    model.save(os.path.join(MODEL_DIR, f\"islr_model__fold_{fold_num+1:>02}__{model.evaluate(val_x, val_y, verbose=0)[1]:.5f}\").replace(\"0.\", \"\"))\n    \n    # Cleanup \n    del model, train_x, train_y, val_x, val_y; gc.collect(); gc.collect();","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:31:52.349699Z","iopub.execute_input":"2023-03-23T17:31:52.350618Z","iopub.status.idle":"2023-03-23T17:32:19.525241Z","shell.execute_reply.started":"2023-03-23T17:31:52.350576Z","shell.execute_reply":"2023-03-23T17:32:19.524122Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\n\n... STARTING TRAINING FOR FOLD #1 ...\n\n\n\nFIRST FOLD... PRINTING MODEL SUMMARY:\n\nModel: \"model_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_8 (InputLayer)        [(None, 472)]             0         \n                                                                 \n dense_21 (Dense)            (None, 512)               242176    \n                                                                 \n batch_normalization_14 (Bat  (None, 512)              2048      \n chNormalization)                                                \n                                                                 \n activation_14 (Activation)  (None, 512)               0         \n                                                                 \n dropout_14 (Dropout)        (None, 512)               0         \n                                                                 \n dense_22 (Dense)            (None, 409)               209817    \n                                                                 \n batch_normalization_15 (Bat  (None, 409)              1636      \n chNormalization)                                                \n                                                                 \n activation_15 (Activation)  (None, 409)               0         \n                                                                 \n dropout_15 (Dropout)        (None, 409)               0         \n                                                                 \n dense_23 (Dense)            (None, 250)               102500    \n                                                                 \n=================================================================\nTotal params: 558,177\nTrainable params: 556,335\nNon-trainable params: 1,842\n_________________________________________________________________\n\n\n... BEGINNING MODEL TRAINING ...\n\n\n\n... STARTING TRAINING FOR FOLD #2 ...\n\n\n\n... BEGINNING MODEL TRAINING ...\n\n\n\n... STARTING TRAINING FOR FOLD #3 ...\n\n\n\n... BEGINNING MODEL TRAINING ...\n\n\n\n... STARTING TRAINING FOR FOLD #4 ...\n\n\n\n... BEGINNING MODEL TRAINING ...\n\n\n\n... STARTING TRAINING FOR FOLD #5 ...\n\n\n\n... BEGINNING MODEL TRAINING ...\n\n\n\n... STARTING TRAINING FOR FOLD #6 ...\n\n\n\n... BEGINNING MODEL TRAINING ...\n\n\n\n... STARTING TRAINING FOR FOLD #7 ...\n\n\n\n... BEGINNING MODEL TRAINING ...\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#CREATE THE PREPROCESSING FUNCTION\ndef dumb_tf_mean(x, axis=None):\n    return tf.math.reduce_mean(x, axis=axis)\n\ndef dumb_tf_std(x, axis=None):\n    x = tf.experimental.numpy.var(x, axis=axis, dtype=tf.float32, ddof=1)\n    return tf.experimental.numpy.sqrt(x)\n\nclass PrepInputs(tf.keras.layers.Layer):\n    def __init__(self, lh_idx_range=(468, 489), pose_idx_range=(489, 522), rh_idx_range=(522, 543), distribution_mean=all_mean, distribution_std=all_std):\n        super(PrepInputs, self).__init__()\n        self.lips = tf.constant([61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308])\n        self.idx_ranges = [lh_idx_range, pose_idx_range, rh_idx_range]\n        self.flat_feat_lens = [2*self.lips.shape[0],]+[2*(_range[1]-_range[0]) for _range in self.idx_ranges]\n        self.distribution_mean = tf.constant(distribution_mean, dtype=tf.float32)\n        self.distribution_std  = tf.constant(distribution_std, dtype=tf.float32)\n    \n    def call(self, x_in):\n        \n        # Split the single vector into 4\n        xs = [tf.gather(x_in[..., :2], self.lips, axis=1),]+[x_in[:, _range[0]:_range[1], :2] for _range in self.idx_ranges]\n        \n        # Reshape based on specific number of keypoints\n        xs = [tf.reshape(_x, (-1, flat_feat_len)) for _x, flat_feat_len in zip(xs, self.flat_feat_lens)]\n        \n        # Drop empty rows - Empty rows are present in \n        #   --> face, lh, rh\n        #   --> so we don't have to for face\n        xs = [tf.boolean_mask(_x, tf.reduce_all(tf.logical_not(tf.math.is_nan(_x)), axis=1), axis=0) for _x in xs]\n        \n        # Get means and stds\n        x_means = [dumb_tf_mean(_x, axis=0) for _x in xs]\n        x_stds  = [dumb_tf_std(_x,  axis=0) for _x in xs]\n        \n        x_out = tf.concat([*x_means, *x_stds], axis=0)\n        x_out = tf.expand_dims(tf.where(tf.math.is_nan(x_out), tf.zeros_like(x_out), x_out), axis=0)\n        x_out = self.standardize_tensor(x_out)\n        return x_out\n    \n    def standardize_tensor(self, tensor):\n        return tf.where(tensor!=0, (tensor-self.distribution_mean)/self.distribution_std, tf.zeros_like(tensor))\n    \np_demo = PrepInputs()(load_relevant_data_subset(train_df.path[0]))\nprint(p_demo.shape)\ndef dumb_tf_mean(x, axis=None):\n    return tf.math.reduce_mean(x, axis=axis)\n\ndef dumb_tf_std(x, axis=None):\n    x = tf.experimental.numpy.var(x, axis=axis, dtype=tf.float32, ddof=1)\n    return tf.experimental.numpy.sqrt(x)\n\nclass PrepInputs(tf.keras.layers.Layer):\n    def __init__(self, lh_idx_range=(468, 489), pose_idx_range=(489, 522), rh_idx_range=(522, 543), distribution_mean=all_mean, distribution_std=all_std):\n        super(PrepInputs, self).__init__()\n        self.lips = tf.constant([61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308])\n        self.idx_ranges = [lh_idx_range, pose_idx_range, rh_idx_range]\n        self.flat_feat_lens = [2*self.lips.shape[0],]+[2*(_range[1]-_range[0]) for _range in self.idx_ranges]\n        self.distribution_mean = tf.constant(distribution_mean, dtype=tf.float32)\n        self.distribution_std  = tf.constant(distribution_std, dtype=tf.float32)\n    \n    def call(self, x_in):\n        \n        # Split the single vector into 4\n        xs = [tf.gather(x_in[..., :2], self.lips, axis=1),]+[x_in[:, _range[0]:_range[1], :2] for _range in self.idx_ranges]\n        \n        # Reshape based on specific number of keypoints\n        xs = [tf.reshape(_x, (-1, flat_feat_len)) for _x, flat_feat_len in zip(xs, self.flat_feat_lens)]\n        \n        # Drop empty rows - Empty rows are present in \n        #   --> face, lh, rh\n        #   --> so we don't have to for face\n        xs = [tf.boolean_mask(_x, tf.reduce_all(tf.logical_not(tf.math.is_nan(_x)), axis=1), axis=0) for _x in xs]\n        \n        # Get means and stds\n        x_means = [dumb_tf_mean(_x, axis=0) for _x in xs]\n        x_stds  = [dumb_tf_std(_x,  axis=0) for _x in xs]\n        \n        x_out = tf.concat([*x_means, *x_stds], axis=0)\n        x_out = tf.expand_dims(tf.where(tf.math.is_nan(x_out), tf.zeros_like(x_out), x_out), axis=0)\n        x_out = self.standardize_tensor(x_out)\n        return x_out\n    \n    def standardize_tensor(self, tensor):\n        return tf.where(tensor!=0, (tensor-self.distribution_mean)/self.distribution_std, tf.zeros_like(tensor))\n    \np_demo = PrepInputs()(load_relevant_data_subset(train_df.path[0]))\nprint(p_demo.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:19.529269Z","iopub.execute_input":"2023-03-23T17:32:19.529924Z","iopub.status.idle":"2023-03-23T17:32:19.674421Z","shell.execute_reply.started":"2023-03-23T17:32:19.529873Z","shell.execute_reply":"2023-03-23T17:32:19.673432Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"(1, 472)\n(1, 472)\n","output_type":"stream"}]},{"cell_type":"code","source":"MODEL_PATHS = sorted(glob(os.path.join(MODEL_DIR, \"*\")), key=lambda x: int(x.rsplit(\"__\", 1)[-1]), reverse=True)\n\nclass ISLRModel(tf.keras.Model):\n    \"\"\"\n    TensorFlow Lite model that takes input tensors and applies:\n        – a preprocessing model\n        – the ISLR model \n    \"\"\"\n\n    def __init__(self, islr_fold_models):\n        \"\"\"\n        Initializes the TFLiteModel with the specified preprocessing model and ISLR model.\n        \"\"\"\n        super(ISLRModel, self).__init__()\n\n        # Load the feature generation and main models\n        self.islr_fold_models  = list(islr_fold_models.values())\n        self.model_weights = tf.repeat(tf.expand_dims(tf.constant([float(k)/100_000. for k in islr_fold_models.keys()], dtype=tf.float32), axis=-1), 250, axis=-1)\n    \n    def __call__(self, inputs, training=None):\n        \"\"\"\n        Applies the feature generation model and main model to the input tensors.\n\n        Args:\n            inputs: Input tensor with shape [batch_size, 543, 3].\n\n        Returns:\n            A dictionary with a single key 'outputs' and corresponding output tensor.\n        \"\"\"\n        batch_size = tf.shape(inputs)[0]\n        outputs    = tf.concat([_model(inputs, training=training) for _model in self.islr_fold_models], axis=0)\n        outputs = tf.reduce_mean(outputs, axis=0, keepdims=True)\n\n        ### Compute the weighted sum and the sum of the weights and compute the weighted mean\n        #outputs    = tf.stack([_model(inputs, training=training) for _model in self.islr_fold_models], axis=1)\n        #outputs = tf.reduce_sum(tf.multiply(outputs, tf.repeat(tf.expand_dims(self.model_weights, axis=0), batch_size, axis=0)), axis=1)\n        #outputs = tf.divide(outputs, tf.reduce_sum(tf.repeat(tf.expand_dims(self.model_weights, axis=0), batch_size, axis=0), axis=1))\n        \n        # Return a dictionary with the output tensor\n        return outputs\n\nclass TFLiteModel(tf.Module):\n    \"\"\"\n    TensorFlow Lite model that takes input tensors and applies:\n        – a preprocessing model\n        – the ISLR model \n    \"\"\"\n\n    def __init__(self, islr_fold_models, islr_fold_pp_fn):\n        \"\"\"\n        Initializes the TFLiteModel with the specified preprocessing model and ISLR model.\n        \"\"\"\n        super(TFLiteModel, self).__init__()\n\n        # Load the feature generation and main models\n        self.prep_inputs = islr_fold_pp_fn()\n        self.islr_fold_models  = list(islr_fold_models.values())\n        self.model_weights = tf.repeat(tf.expand_dims(tf.constant([float(k)/100_000. for k in islr_fold_models.keys()], dtype=tf.float32), axis=-1), 250, axis=-1)\n    \n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs):\n        \"\"\"\n        Applies the feature generation model and main model to the input tensors.\n\n        Args:\n            inputs: Input tensor with shape [batch_size, 543, 3].\n\n        Returns:\n            A dictionary with a single key 'outputs' and corresponding output tensor.\n        \"\"\"\n        x = self.prep_inputs(tf.cast(inputs, dtype=tf.float32))\n        outputs  = tf.concat([_model(x) for _model in self.islr_fold_models], axis=0)\n        \n        # Compute the weighted sum and the sum of the weights and compute the weighted mean\n        outputs = tf.reduce_sum(tf.multiply(outputs, self.model_weights), axis=0)\n        outputs = tf.divide(outputs, tf.reduce_sum(self.model_weights, axis=0))\n        \n        # Return a dictionary with the output tensor\n        return {'outputs': outputs}\n\nISLR_FOLD_MODELS = {_path.rsplit(\"__\", 1)[-1]:tf.keras.models.load_model(_path, compile=False) for _path in MODEL_PATHS}\ntflite_keras_model = TFLiteModel(ISLR_FOLD_MODELS, PrepInputs)\nout = tflite_keras_model(load_relevant_data_subset(train_df.path[0]))[\"outputs\"]\nnp.argmax(out)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:19.676029Z","iopub.execute_input":"2023-03-23T17:32:19.676390Z","iopub.status.idle":"2023-03-23T17:32:24.040591Z","shell.execute_reply.started":"2023-03-23T17:32:19.676352Z","shell.execute_reply":"2023-03-23T17:32:24.038057Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"25"},"metadata":{}}]},{"cell_type":"code","source":"def get_input_shape(num_frames, landmarks, flag_drop_z):\n    input_shape = (num_frames, landmarks * 3)\n\n    if flag_drop_z:\n        num_coords = 2\n    else:\n        num_coords = 3\n\n    return (num_frames, landmarks * num_coords)\n\noutput_bias = tf.keras.initializers.Constant(1.0 / 250.0)\nclass MSD(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        units,\n        fold_num=1,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.lin = tf.keras.layers.Dense(\n            units,\n            activation=None,\n            use_bias=True,\n            bias_initializer=output_bias,\n            # kernel_regularizer=R.l2(WEIGHT_REGULARIZE)\n        )\n\n        rate_dropout = 0.5\n        self.dropouts = [\n            tf.keras.layers.Dropout((rate_dropout - 0.2), seed=135 + fold_num),\n            tf.keras.layers.Dropout((rate_dropout - 0.1), seed=690 + fold_num),\n            tf.keras.layers.Dropout((rate_dropout), seed=275 + fold_num),\n            tf.keras.layers.Dropout((rate_dropout + 0.1), seed=348 + fold_num),\n            tf.keras.layers.Dropout((rate_dropout + 0.2), seed=861 + fold_num),\n        ]\n\n    def call(self, inputs):\n        for ii, drop in enumerate(self.dropouts):\n            if ii == 0:\n                out = self.lin(drop(inputs)) / 5.0\n            else:\n                out += self.lin(drop(inputs)) / 5.0\n        return out\n\n\nclass ResidualBlock(tf.keras.layers.Layer):\n    def __init__(self, units, dropout):\n        super().__init__()\n        self.linear = tf.keras.layers.Dense(units)\n        self.bn = tf.keras.layers.BatchNormalization()\n        self.act = tf.keras.layers.Activation(\"gelu\")\n        if dropout != 0:\n            self.drop = tf.keras.layers.Dropout(dropout)\n            self.flag_use_drop = True\n        else:\n            self.flag_use_drop = False\n\n    def call(self, x):\n        x = self.linear(x)\n        x = self.bn(x)\n        x = self.act(x)\n        if self.flag_use_drop:\n            x = self.drop(x)\n        return x\n\nclass GRUModel(tf.keras.layers.Layer):\n    def __init__(self, units, dropout, num_blocks):\n        super().__init__()\n        self.start_gru = tf.keras.layers.GRU(\n            units=units, dropout=0.0, return_sequences=True\n        )\n        self.end_gru = tf.keras.layers.GRU(\n            units=units, dropout=dropout, return_sequences=False\n        )\n\n        if (num_blocks - 2) > 0:\n            self.gru_blocks = [\n                tf.keras.layers.GRU(units=units, dropout=dropout, return_sequences=True)\n                * (num_blocks - 2)\n            ]\n            self.flag_use_gru_blocks = True\n        else:\n            self.flag_use_gru_blocks = False\n\n    def call(self, x):\n        x = self.start_gru(x)\n        if self.flag_use_gru_blocks:\n            for blk in self.gru_blocks:\n                x = blk(x)\n        x = self.end_gru(x)\n        return x\n\ndef model_utils(cfg, fold_num):\n    metric_ls = [\n        tf.keras.metrics.SparseCategoricalAccuracy(),\n        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5),\n    ]\n\n    cb_list = [\n        tf.keras.callbacks.EarlyStopping(\n            patience=5,\n            restore_best_weights=True,\n            verbose=1,\n            monitor=cfg[\"TARGET_METRIC\"],\n        ),\n        tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.8, verbose=1),\n        tf.keras.callbacks.ModelCheckpoint(\n            f\"{SAVE_DIR}/best_acc_{fold_num}.h5\",\n            monitor=cfg[\"TARGET_METRIC\"],\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=True,\n            mode=\"max\",\n            save_freq=\"epoch\",\n        ),\n    ]\n\n    if cfg[\"FLAG_WANDB\"]:\n        cb_list += [#WandbMetricsLogger()\n            WandbCallback(\n                monitor=cfg[\"TARGET_METRIC\"],\n                log_weights=False,\n                log_evaluation=False,\n                save_model=False,\n            )\n        ]\n\n    opt = tfa.optimizers.AdamW(weight_decay=0, learning_rate=cfg[\"LR\"])\n    # opt = tf.keras.optimizers.Adam(learning_rate=LR)\n    # opt = tfa.optimizers.RectifiedAdam(learning_rate=LR)\n    # opt = tfa.optimizers.Lookahead(opt, sync_period=5)\n\n    return metric_ls, cb_list, opt\n\n# Analyzing Handedness\nleft_handed_signer = [16069, 32319, 36257, 22343, 27610, 61333, 34503, 55372, 37055]  # both_hands_signer-> 37055\nright_handed_signer = [26734, 28656, 25571, 62590, 29302, 49445, 53618, 18796, 4718, 2044, 37779, 30680,]\nlip_landmarks = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 146, 91, 181, 84, 17, 314, 405, 321, 375, 78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n\ndi = {}\nfor k in left_handed_signer:\n    di[k] = 0\nfor k in right_handed_signer:\n    di[k] = 1\n\nleft_hand_landmarks = list(range(468, 468 + 21))\nright_hand_landmarks = list(range(522, 522 + 21))\n\naveraging_sets = [\n    [0, 468],\n    [489, 33],\n]  ## average over the entire face, and the entire 'pose'\n\npoint_landmarks = [\n    item\n    for sublist in [lip_landmarks, left_hand_landmarks, right_hand_landmarks]\n    for item in sublist\n]\n\nLANDMARKS = len(point_landmarks) #+ len(averaging_sets)\n\n# Fixed  ##################################################################################\n\nFLAG_DROP_Z = False\nROWS_PER_FRAME = 543\nNUM_FRAMES = 15\nINPUT_SHAPE = get_input_shape(NUM_FRAMES, LANDMARKS, FLAG_DROP_Z)\nSEGMENTS = 3\nNUM_BASE_FEATS = (SEGMENTS + 1) * INPUT_SHAPE[1] * 2\nFLAT_FRAME_SHAPE = NUM_BASE_FEATS + (INPUT_SHAPE[0] * INPUT_SHAPE[1])\ndecoder = {v: k for k, v in read_json_file(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\").items()}\n\n    \n_inputs = tf.keras.layers.Input(shape=(FLAT_FRAME_SHAPE,))\n\n# import ipdb\n# ipdb.set_trace()\nx = _inputs[:, :NUM_BASE_FEATS]\nx_conv = tf.reshape(_inputs[:, NUM_BASE_FEATS:], (-1, NUM_FRAMES, INPUT_SHAPE[1]))\n\n# Concat Dilated Convolutions with actual data\ngru_out = GRUModel(512, 0.5, 1)(x_conv)\nx = gru_out\n\n# Residual Block\nx = ResidualBlock(1024, 0.25)(x)\nx += ResidualBlock(1024, 0.0)(x)\n\n# Final output MSD Layer\nx = MSD(units=250)(x)\n_outputs = tf.keras.layers.Softmax(dtype=\"float32\")(x)\n\n# Build the model\ngwg_models=[]\nfor i in range(3):\n    gwg_model = tf.keras.models.Model(inputs=_inputs, outputs=_outputs)\n    gwg_model.summary()\n    gwg_model.load_weights(f\"/kaggle/input/islr-external-3-my/models/best_acc_{i}.h5\")\n    gwg_models.append(gwg_model)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:24.044379Z","iopub.execute_input":"2023-03-23T17:32:24.046049Z","iopub.status.idle":"2023-03-23T17:32:26.039208Z","shell.execute_reply.started":"2023-03-23T17:32:24.046008Z","shell.execute_reply":"2023-03-23T17:32:26.038150Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model: \"model_14\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_15 (InputLayer)          [(None, 5658)]       0           []                               \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 3690)        0           ['input_15[0][0]']               \n icingOpLambda)                                                                                   \n                                                                                                  \n tf.reshape (TFOpLambda)        (None, 15, 246)      0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n gru_model (GRUModel)           (None, 512)          2743296     ['tf.reshape[0][0]']             \n                                                                                                  \n residual_block (ResidualBlock)  (None, 1024)        529408      ['gru_model[0][0]']              \n                                                                                                  \n residual_block_1 (ResidualBloc  (None, 1024)        1053696     ['residual_block[0][0]']         \n k)                                                                                               \n                                                                                                  \n tf.__operators__.add (TFOpLamb  (None, 1024)        0           ['residual_block[0][0]',         \n da)                                                              'residual_block_1[0][0]']       \n                                                                                                  \n msd (MSD)                      (None, 250)          256250      ['tf.__operators__.add[0][0]']   \n                                                                                                  \n softmax (Softmax)              (None, 250)          0           ['msd[0][0]']                    \n                                                                                                  \n==================================================================================================\nTotal params: 4,582,650\nTrainable params: 4,578,554\nNon-trainable params: 4,096\n__________________________________________________________________________________________________\nModel: \"model_15\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_15 (InputLayer)          [(None, 5658)]       0           []                               \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 3690)        0           ['input_15[0][0]']               \n icingOpLambda)                                                                                   \n                                                                                                  \n tf.reshape (TFOpLambda)        (None, 15, 246)      0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n gru_model (GRUModel)           (None, 512)          2743296     ['tf.reshape[0][0]']             \n                                                                                                  \n residual_block (ResidualBlock)  (None, 1024)        529408      ['gru_model[0][0]']              \n                                                                                                  \n residual_block_1 (ResidualBloc  (None, 1024)        1053696     ['residual_block[0][0]']         \n k)                                                                                               \n                                                                                                  \n tf.__operators__.add (TFOpLamb  (None, 1024)        0           ['residual_block[0][0]',         \n da)                                                              'residual_block_1[0][0]']       \n                                                                                                  \n msd (MSD)                      (None, 250)          256250      ['tf.__operators__.add[0][0]']   \n                                                                                                  \n softmax (Softmax)              (None, 250)          0           ['msd[0][0]']                    \n                                                                                                  \n==================================================================================================\nTotal params: 4,582,650\nTrainable params: 4,578,554\nNon-trainable params: 4,096\n__________________________________________________________________________________________________\nModel: \"model_16\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_15 (InputLayer)          [(None, 5658)]       0           []                               \n                                                                                                  \n tf.__operators__.getitem_1 (Sl  (None, 3690)        0           ['input_15[0][0]']               \n icingOpLambda)                                                                                   \n                                                                                                  \n tf.reshape (TFOpLambda)        (None, 15, 246)      0           ['tf.__operators__.getitem_1[0][0\n                                                                 ]']                              \n                                                                                                  \n gru_model (GRUModel)           (None, 512)          2743296     ['tf.reshape[0][0]']             \n                                                                                                  \n residual_block (ResidualBlock)  (None, 1024)        529408      ['gru_model[0][0]']              \n                                                                                                  \n residual_block_1 (ResidualBloc  (None, 1024)        1053696     ['residual_block[0][0]']         \n k)                                                                                               \n                                                                                                  \n tf.__operators__.add (TFOpLamb  (None, 1024)        0           ['residual_block[0][0]',         \n da)                                                              'residual_block_1[0][0]']       \n                                                                                                  \n msd (MSD)                      (None, 250)          256250      ['tf.__operators__.add[0][0]']   \n                                                                                                  \n softmax (Softmax)              (None, 250)          0           ['msd[0][0]']                    \n                                                                                                  \n==================================================================================================\nTotal params: 4,582,650\nTrainable params: 4,578,554\nNon-trainable params: 4,096\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def tf_nan_mean(x, axis=0):\n    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis)\n\ndef tf_nan_std(x, axis=0):\n    d = x - tf_nan_mean(x, axis=axis)\n    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis))\n\ndef flatten_means_and_stds(x, axis=0):\n    # Get means and stds\n    x_mean = tf_nan_mean(x, axis=0)\n    x_std  = tf_nan_std(x,  axis=0)\n\n    x_out = tf.concat([x_mean, x_std], axis=0)\n    x_out = tf.reshape(x_out, (1, INPUT_SHAPE[1]*2))\n    x_out = tf.where(tf.math.is_finite(x_out), x_out, tf.zeros_like(x_out))\n    return x_out\n\nclass FeatureGen_1(tf.keras.layers.Layer):\n    def __init__(self):\n        super(FeatureGen_1, self).__init__()\n    \n    def call(self, x_in):\n        x = tf.gather(x_in, point_landmarks, axis=1)\n\n        x_padded = x\n        for i in range(SEGMENTS):\n            p0 = tf.where( ((tf.shape(x_padded)[0] % SEGMENTS) > 0) & ((i % 2) != 0) , 1, 0)\n            p1 = tf.where( ((tf.shape(x_padded)[0] % SEGMENTS) > 0) & ((i % 2) == 0) , 1, 0)\n            paddings = [[p0, p1], [0, 0], [0, 0]]\n            x_padded = tf.pad(x_padded, paddings, mode=\"SYMMETRIC\")\n        x_list = tf.split(x_padded, SEGMENTS)\n        x_list = [flatten_means_and_stds(_x, axis=0) for _x in x_list]\n\n        x_list.append(flatten_means_and_stds(x, axis=0))\n        \n        ## Resize only dimension 0. Resize can't handle nan, so replace nan with that dimension's avg value to reduce impact.\n        x = tf.image.resize(tf.where(tf.math.is_finite(x), x, tf_nan_mean(x, axis=0)), [NUM_FRAMES, LANDMARKS])\n        x = tf.reshape(x, (1, INPUT_SHAPE[0]*INPUT_SHAPE[1]))\n        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n        x_list.append(x)\n        x = tf.concat(x_list, axis=1)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:26.040960Z","iopub.execute_input":"2023-03-23T17:32:26.041754Z","iopub.status.idle":"2023-03-23T17:32:26.056489Z","shell.execute_reply.started":"2023-03-23T17:32:26.041710Z","shell.execute_reply":"2023-03-23T17:32:26.055243Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"ROWS_PER_FRAME = 543\nLANDMARK_IDX = [0,9,11,13,14,17,117,118,119,199,346,347,348] + list(range(468,543))\nltsm_model = tf.keras.models.load_model('/kaggle/input/sign-language-classification-2idat/lstm.h5')","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:26.062306Z","iopub.execute_input":"2023-03-23T17:32:26.062882Z","iopub.status.idle":"2023-03-23T17:32:27.119822Z","shell.execute_reply.started":"2023-03-23T17:32:26.062841Z","shell.execute_reply":"2023-03-23T17:32:27.118742Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"DROP_Z2 = False\n\nNUM_FRAMES2 = 15\nSEGMENTS2 = 3\n\nLEFT_HAND_OFFSET2 = 468\nPOSE_OFFSET2 = LEFT_HAND_OFFSET2 + 21\nRIGHT_HAND_OFFSET2 = POSE_OFFSET2 + 33\n\n## average over the entire face, and the entire 'pose'\naveraging_sets2 = [[0, 468], [POSE_OFFSET2, 33]]\n\nlip_landmarks2 = [\n    61,\n    185,\n    40,\n    39,\n    37,\n    0,\n    267,\n    269,\n    270,\n    409,\n    291,\n    146,\n    91,\n    181,\n    84,\n    17,\n    314,\n    405,\n    321,\n    375,\n    78,\n    191,\n    80,\n    81,\n    82,\n    13,\n    312,\n    311,\n    310,\n    415,\n    95,\n    88,\n    178,\n    87,\n    14,\n    317,\n    402,\n    318,\n    324,\n    308,\n]\nleft_hand_landmarks2 = list(range(LEFT_HAND_OFFSET2, LEFT_HAND_OFFSET2 + 21))\nright_hand_landmarks2 = list(range(RIGHT_HAND_OFFSET2, RIGHT_HAND_OFFSET2 + 21))\n\npoint_landmarks2 = [\n    item\n    for sublist in [lip_landmarks2, left_hand_landmarks2, right_hand_landmarks2]\n    for item in sublist\n]\n\nLANDMARKS2 = len(point_landmarks2) + len(averaging_sets2)\nprint(LANDMARKS)\nif DROP_Z2:\n    INPUT_SHAPE2 = (NUM_FRAMES2, LANDMARKS2 * 2)\nelse:\n    INPUT_SHAPE2 = (NUM_FRAMES2, LANDMARKS2 * 3)\n\nFLAT_INPUT_SHAPE2 = (INPUT_SHAPE2[0] + 2 * (SEGMENTS2 + 1)) * INPUT_SHAPE2[1]\n\ndef flatten_means_and_stds2(x, axis=0):\n    # Get means and stds\n    x_mean = tf_nan_mean(x, axis=0)\n    x_std = tf_nan_std(x, axis=0)\n\n    x_out = tf.concat([x_mean, x_std], axis=0)\n    x_out = tf.reshape(x_out, (1, INPUT_SHAPE2[1] * 2))\n    x_out = tf.where(tf.math.is_finite(x_out), x_out, tf.zeros_like(x_out))\n    return x_out\n\nclass FeatureGenTF(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, x_in):\n        if DROP_Z:\n            x_in = x_in[:, :, 0:2]\n        x_list = [\n            tf.expand_dims(\n                tf_nan_mean(x_in[:, av_set[0] : av_set[0] + av_set[1], :], axis=1),\n                axis=1,\n            )\n            for av_set in averaging_sets\n        ]\n        x_list.append(tf.gather(x_in, point_landmarks, axis=1))\n        x = tf.concat(x_list, 1)\n\n        x_padded = x\n        for i in range(SEGMENTS2):\n            p0 = tf.where(\n                ((tf.shape(x_padded)[0] % SEGMENTS2) > 0) & ((i % 2) != 0), 1, 0\n            )\n            p1 = tf.where(\n                ((tf.shape(x_padded)[0] % SEGMENTS2) > 0) & ((i % 2) == 0), 1, 0\n            )\n            paddings = [[p0, p1], [0, 0], [0, 0]]\n            x_padded = tf.pad(x_padded, paddings, mode=\"SYMMETRIC\")\n        x_list = tf.split(x_padded, SEGMENTS2)\n        x_list = [flatten_means_and_stds2(_x, axis=0) for _x in x_list]\n\n        x_list.append(flatten_means_and_stds2(x, axis=0))\n\n        ## Resize only dimension 0. Resize can't handle nan, so replace nan with that dimension's avg value to reduce impact.\n        x = tf.image.resize(\n            tf.where(tf.math.is_finite(x), x, tf_nan_mean(x, axis=0)),\n            [NUM_FRAMES2, LANDMARKS2],\n        )\n        x = tf.reshape(x, (1, INPUT_SHAPE2[0] * INPUT_SHAPE2[1]))\n        x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n        x_list.append(x)\n        x = tf.concat(x_list, axis=1)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:27.124033Z","iopub.execute_input":"2023-03-23T17:32:27.124343Z","iopub.status.idle":"2023-03-23T17:32:27.143553Z","shell.execute_reply.started":"2023-03-23T17:32:27.124313Z","shell.execute_reply":"2023-03-23T17:32:27.142358Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"82\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Generate TFLite Model and submission.zip","metadata":{}},{"cell_type":"code","source":"class TFLiteModel(tf.Module):\n    \"\"\"\n    TensorFlow Lite model that takes input tensors and applies:\n        – a preprocessing model\n        – the ISLR model \n    \"\"\"\n\n    def __init__(self, islr_fold_models, islr_fold_pp_fn, gwg_model, gwg_pp_fn, asl_models, ltsm_model, ASLInferModel):\n        \"\"\"\n        Initializes the TFLiteModel with the specified preprocessing model and ISLR model.\n        \"\"\"\n        super(TFLiteModel, self).__init__()\n\n        # Load the feature generation and main models\n        self.prep_inputs_1 = islr_fold_pp_fn()\n        self.prep_inputs_2 = gwg_pp_fn()\n        self.models_1      = list(islr_fold_models.values())\n        self.models_2       = gwg_models\n        self.prep_inputs = FeatureGen()\n        self.asl_models   = asl_models\n        self.ltsm_model   = ltsm_model\n        self.feature_gen = FeatureGenTF()\n        self.ASLInferModel = tf.saved_model.load(ASLInferModel)\n        self.feature_gen.trainable = False\n        self.ASLInferModel.trainable = False\n    \n    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')])\n    def __call__(self, inputs):\n        \"\"\"\n        Applies the feature generation model and main model to the input tensors.\n\n        Args:\n            inputs: Input tensor with shape [batch_size, 543, 3].\n\n        Returns:\n            A dictionary with a single key 'outputs' and corresponding output tensor.\n        \"\"\"\n        x = self.prep_inputs(tf.cast(inputs, dtype=tf.float32))\n        outputs=[]\n        for asl_model in self.asl_models:\n            outputs.append(asl_model(x)[0, :])\n        outputs_3 = tf.keras.layers.Average()(outputs)\n        \n        \n        x1 = self.prep_inputs_1(tf.cast(inputs, dtype=tf.float32))\n        x2 = self.prep_inputs_2(tf.cast(inputs, dtype=tf.float32))\n        \n        outputs_1 = tf.keras.layers.Average()([_model(x1) for _model in self.models_1])\n        \n        outputs_2=[]\n        for gwg_model in self.models_2:\n            outputs_2.append(gwg_model(x2))\n        outputs_2 = tf.keras.layers.Average()(outputs_2)\n        \n        # 2x weighting higher score via repeat 14\n        outputs = tf.multiply(0.25, outputs_1)+tf.multiply(0.45, outputs_2)+tf.multiply(0.3, outputs_3)\n        \n        x_ltsm = tf.gather(inputs, LANDMARK_IDX, axis=1)\n\n        # fill nan\n        x_ltsm = tf.where(tf.math.is_nan(x_ltsm), tf.zeros_like(x_ltsm), x_ltsm)\n\n        # flatten landmark xyz coordinates ()\n        x_ltsm = tf.concat([x_ltsm[...,i] for i in range(3)], -1)\n\n        x_ltsm = tf.expand_dims(x_ltsm,0)\n\n        # call trained model\n        out_x_ltsm = self.ltsm_model(x_ltsm)\n        \n        outputs = tf.multiply(0.70, outputs)+tf.multiply(0.30, out_x_ltsm)\n        \n        \n        features = self.feature_gen(tf.cast(inputs, dtype=tf.float32))\n\n        outputs_5 = self.ASLInferModel(**{\"input\": features})[\"output\"][0, :]\n        \n        outputs = tf.multiply(0.95, outputs)+tf.multiply(0.05, outputs_5)\n        \n        # Return a dictionary with the output tensor\n        return {'outputs': outputs}\n    \nISLR_FOLD_MODELS = {_path.rsplit(\"__\", 1)[-1]:tf.keras.models.load_model(_path, compile=False) for _path in MODEL_PATHS}\ntflite_keras_model = TFLiteModel(ISLR_FOLD_MODELS, PrepInputs, gwg_models, FeatureGen_1,models[2:3], ltsm_model, '/kaggle/input/asl-sign-detection-pytorch-lightning/tf_model')\nout = tflite_keras_model(load_relevant_data_subset(train_df.path[0]))[\"outputs\"]\nnp.argmax(out)","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:27.145453Z","iopub.execute_input":"2023-03-23T17:32:27.146109Z","iopub.status.idle":"2023-03-23T17:32:38.391656Z","shell.execute_reply.started":"2023-03-23T17:32:27.146069Z","shell.execute_reply":"2023-03-23T17:32:38.390679Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"25"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, our ensemble includes the following models:\n* ISLR_FOLD_MODELS -https://www.kaggle.com/code/dschettler8845/gislr-how-to-ensemble\n* gwg_models - https://www.kaggle.com/code/dschettler8845/gislr-how-to-ensemble\n* my models - https://www.kaggle.com/code/aikhmelnytskyy/gislr-tf-on-the-shoulders-ensamble?scriptVersionId=121543912\n* ltsm_model - https://www.kaggle.com/code/aleksandrkruchinin/tflite-ensemble","metadata":{}},{"cell_type":"code","source":"!pip install tflite-runtime\nimport tflite_runtime.interpreter as tflite","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:38.393137Z","iopub.execute_input":"2023-03-23T17:32:38.393517Z","iopub.status.idle":"2023-03-23T17:32:48.587546Z","shell.execute_reply.started":"2023-03-23T17:32:38.393479Z","shell.execute_reply":"2023-03-23T17:32:48.586348Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Collecting tflite-runtime\n  Downloading tflite_runtime-2.11.0-cp37-cp37m-manylinux2014_x86_64.whl (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.19.2 in /opt/conda/lib/python3.7/site-packages (from tflite-runtime) (1.21.6)\nInstalling collected packages: tflite-runtime\nSuccessfully installed tflite-runtime-2.11.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\nkeras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = keras_model_converter.convert()\nwith open('/kaggle/working/models/model.tflite', 'wb') as f:\n    f.write(tflite_model)\n!zip submission.zip /kaggle/working/models/model.tflite\n\ninterpreter = tflite.Interpreter(\"/kaggle/working/models/model.tflite\")\nfound_signatures = list(interpreter.get_signature_list().keys())\n# if REQUIRED_SIGNATURE not in found_signatures:\n#     raise KernelEvalException('Required input signature not found.')\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\n\noutput = prediction_fn(inputs=load_relevant_data_subset(train_df.path[0]))\nsign = np.argmax(output[\"outputs\"])\n\nprint(\"PRED : \", decoder[sign])\nprint(\"GT   : \", train_df.sign[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-23T17:32:48.589840Z","iopub.execute_input":"2023-03-23T17:32:48.590238Z","iopub.status.idle":"2023-03-23T17:34:03.267289Z","shell.execute_reply.started":"2023-03-23T17:32:48.590194Z","shell.execute_reply":"2023-03-23T17:34:03.265866Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/models/model.tflite (deflated 37%)\nPRED :  blow\nGT   :  blow\n","output_type":"stream"}]}]}